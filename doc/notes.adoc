= Development notes and plans

== Backing storage options to consider

* Plain old filesystem
(probably won‚Äôt scale with the level of caching granularity
we might want to aspire to)

* https://rocksdb.org/[RocksDB] / https://github.com/google/leveldb[LevelDB]

* https://github.com/spacejam/sled[sled]
** Author recommends comparing against PingCAP‚Äôs TitanDB
for the large blob workload:
+
[quote,'http://tylerneely.com/[Tyler Neely] (https://github.com/spacejam[@spacejam])']
____
one DB that might be interesting to benchmark for your workload
against sled is PingCAP's titandb, which separates keys from values,
and I think they might move values even less over time, so that could
be one of the better options

they use the WiscKey DB architecture of keeping keys in an LSM but
values out of it so they can avoid moving values as often. this is the
same approach used in the Badger DB for the Go ecosystem that is nice
in some situations
____
** https://tikv.org/[TiKV] for distributed stores would be interesting,
though probably excessive (there‚Äôs not as much chance of conflict
in a largely content‚Äêaddressed database)

* https://symas.com/lmdb/[LMDB]

* https://sqlite.org/[SQLite]

Investigate which of these options would need
an accompanying large blob storage system (certainly SQLite)
and then don‚Äôt bother using those options
because that would be way too much work.
The sled author‚Äôs remarks on the topic:

[quote,'http://tylerneely.com/[Tyler Neely] (https://github.com/spacejam[@spacejam])']
____
the general reason why most DBs will suggest punting to a FS for that
kind of thing is because over time, databases will tend to rearrange
items internally in order to perform defragmentation etc... sled will
actually just spill over to using a file itself, but as I'm writing this
it will only split leaf nodes when they hit 17 children, so if you have
17 1gb values all clustered on one leaf, sled will need to read that
whole 17gb file in before serving your data

having said that, it isn't that much work to change how sled splits leaf
nodes to increase this granularity, so each 1gb value would get its
own file

DBs usually assume they can copy values over and over pretty cheaply,
and for most DB's this is indeed sort of a nightmare workload

but right now, sled only splits leafs when they exceed the 16 child
limit. in general I want to make this more size based, but as of right
now it's up to 16 values per node, and this stuff happens per-node
rather than per-key

anyway, the bottom line is, measure it üôÇ

as the creator of the thing I'll always be aware of ways of making it better, but maybe for you it's already good enough
____

== Reference format

0x00/`uu:‚Ä¶`:: A universally unique random identifier.
0x01/`b3:‚Ä¶`:: A https://github.com/BLAKE3-team/BLAKE3[BLAKE3] hash.
‚Ä¶:: Extensible! Let‚Äôs hope we never need more than 255 hash functions.

Hash/UUID payload is 31 bytes so that the reference as a whole
is 256 bits.

TODO: Come up with a name for `uu:‚Ä¶` identifiers that doesn‚Äôt clash
with standard 128 bit UUIDs.

----
# Hash a constant
$ mew ref 1234
b3:‚Ä¶

# Import and hash a file
$ mew ref -f ./file
b3:‚Ä¶

# Hash a file without importing into the forest
$ mew ref -n -f ./file
b3:‚Ä¶

# Download and import a file
$ mew ref -f https://example.com/example-1.0.tar.gz
b3:‚Ä¶

# Generate a unique random identifier
$ mew ref -u
uu:‚Ä¶
----

== Investigate network protocol/RPC options

gRPC and Cap‚Äôn Proto are the main contenders here.
Maybe figure out if gRPC could be used with Cap‚Äôn Proto payloads,
or hand‚Äêroll something based on Cap‚Äôn Proto + QUIC/HTTP 3.

=== Compare Rust QUIC/HTTP 3 libraries

* https://github.com/mozilla/neqo
* https://github.com/cloudflare/quiche
* https://github.com/djc/quinn

== Sketch out the mewl language

Somewhere between a purely functional shell
and https://dhall-lang.org/[Dhall];
implementation tightly integrated with the build store interface.

Ensuring adherence to object‚Äêcapability principles is Very Important‚Ñ¢.

=== Typed tree structure sketch

[source,mewl]
----
TreeSpec : Type
Dir : Map PathComponent TreeSpec ‚Üí TreeSpec
Blob : TreeSpec
Executable : TreeSpec

Tree : TreeType ‚Üí Type
get : (p : PathComponent) ‚Üí ‚àÄts ‚áí Tree (Dir ts) ‚Üí (e : p ‚àà ts) ‚áí Tree (value e)
get? : PathComponent ‚Üí ‚àÄts ‚áí Tree (Dir ts) ‚Üí Option (‚àÉt ¬∑ Tree t)
execute : Tree Executable ‚Üí ‚Ä¶
execute? : ‚àÄt ‚áí Tree t ‚Üí Option ‚Ä¶

‚Äî guaranteed to contain bin/hello, which you can execute (including
‚Äî from build rules), and share/doc/hello/README, a plain blob, but can
‚Äî also contain arbitrary other trees in addition
hello-tree : Tree (Dir {
  "bin" = Dir {"hello" = Executable},
  "share" = Dir {"doc" = Dir {"hello" = Dir {"README" = Blob}}},
})

‚Äî hello-tree ‚ñª get "bin" : Tree (Dir {"hello" = Executable})
‚Äî execute (hello-tree ‚ñª get "bin" ‚ñª get "hello") : ‚Ä¶
----

Could probably use row types for this:

[source,mewl]
----
‚Äî essentially mapping from identifiers to the specified type
Row : Type ‚Üí Type

‚¶É
  bin : Dir ‚¶Éhello : Executable‚¶Ñ,
  share : Dir ‚¶Édoc : Dir ‚¶Éhello : Dir ‚¶ÉREADME : Blob‚¶Ñ‚¶Ñ‚¶Ñ,
‚¶Ñ : Row TreeSpec
----

Then we can get nicer `bin : ‚Ä¶` syntax by punning on the fact
that rows would also be used to specify record types.

== Investigate Guix properly

and raid it for ideas.

== Incremental computation for configuration management

http://adapton.org/[Adapton] seems like it should have insights
that are applicable to modelling Ansible/Terraform‚Äêstyle
reconciliation of configuration with state in a pure system.

== Toolchains and bootstrapping

Shiz‚Äôs LLVM + clang + LLD + elftoolchain + compiler-rt + libc++ Linux
https://github.com/Shizmob/aports/commits/system-llvm-elftoolchain[toolchain work]
is probably worth referencing.

== Meta

=== Move nixpkgs{,-mozilla} pins to fetchFromGitHub

Easier to include in `cargo publish` tarballs.

=== Set up a more formal documentation system

And then preferably have it publish to https://mew.build/[mew.build].

=== Set up CI

Investigate Azure Pipelines and GitHub Actions.

=== Set up bors

This will probably be really annoying in the early stages of hacking,
depending on the latency.

It would be good to integrate the bors setup
with https://github.com/spotify/git-test[git-test]
to test all the commits of a pull request
rather than just the HEAD.

=== Set up and require commit signing

See above, though tapping a YubiKey a few times
when pushing to the public repository isn‚Äôt too bad.

=== Move to self‚Äêhosted infrastructure

GitHub https://github.com/drop-ice/dear-github-2.0[supports ICE],
and it would be nice to have the root of trust for binary builds
under our direct control.

This would require manually administering build and VCS machines,
prevent the use of the existing bors implementation,
and substantially increase the barrier to contribution,
so it should be done carefully:
ideally people would still be able to contribute
via GitHub issues and pull requests
and have them automatically mirrored to the self‚Äêhosted infrastructure.

=== Prohibit force pushes

let‚Äôs not get ahead of ourselves here
